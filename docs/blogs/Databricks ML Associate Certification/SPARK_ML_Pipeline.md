Perfect capstone question ğŸ‘
Below is a **complete, end-to-end PySpark ML pipeline** written **exactly in the Databricks ML Associate + Unity Catalog + MLflow way**.

This is **production-grade**, **exam-aligned**, and reusable.

Iâ€™ll cover **everything in one flow**:

1. Read data (numeric + categorical)
2. Feature engineering pipeline
3. Train/test split
4. Train a model
5. Track with MLflow
6. Register model in **Unity Catalog**
7. Create & register **Feature Store table in UC**
8. Reuse features for **retraining**
9. Reuse model + features for **inference**

You can treat this as **the final mental model for the exam**.

---

# 0ï¸âƒ£ Assumptions (Context)

* Spark DataFrame input
* Binary classification example
* Unity Catalog enabled
* Databricks ML Runtime
* Table names are UC-qualified

---

# 1ï¸âƒ£ Read Data (Numeric + Categorical)

```python
df = spark.read.table("main.raw.customer_data")

df.printSchema()
```

Example columns:

* `customer_id` (key)
* `age` (numeric)
* `income` (numeric)
* `country` (categorical)
* `churn` (label)

---

# 2ï¸âƒ£ Feature Engineering Pipeline (Spark ML)

## Separate feature types

```python
numeric_cols = ["age", "income"]
categorical_cols = ["country"]
label_col = "churn"
```

---

## Transformers

```python
from pyspark.ml.feature import (
    StringIndexer,
    OneHotEncoder,
    VectorAssembler,
    Imputer
)
```

### Impute numeric values

```python
imputer = Imputer(
    inputCols=numeric_cols,
    outputCols=[f"{c}_imputed" for c in numeric_cols],
    strategy="median"
)
```

### Encode categorical values

```python
indexer = StringIndexer(
    inputCol="country",
    outputCol="country_idx",
    handleInvalid="keep"
)

encoder = OneHotEncoder(
    inputCol="country_idx",
    outputCol="country_vec"
)
```

### Assemble features

```python
assembler = VectorAssembler(
    inputCols=["age_imputed", "income_imputed", "country_vec"],
    outputCol="features"
)
```

---

# 3ï¸âƒ£ Model (Estimator)

```python
from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression(
    featuresCol="features",
    labelCol=label_col
)
```

---

# 4ï¸âƒ£ Full Pipeline

```python
from pyspark.ml import Pipeline

pipeline = Pipeline(stages=[
    imputer,
    indexer,
    encoder,
    assembler,
    lr
])
```

ğŸ“Œ **Exam key**

> Estimators + transformers chained â†’ Pipeline

---

# 5ï¸âƒ£ Train / Test Split

```python
train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)
```

---

# 6ï¸âƒ£ Train + Track with MLflow

```python
import mlflow
import mlflow.spark

mlflow.set_experiment("/Shared/churn_experiment")

with mlflow.start_run():
    model = pipeline.fit(train_df)

    preds = model.transform(test_df)

    mlflow.spark.log_model(
        spark_model=model,
        artifact_path="model",
        registered_model_name="main.ml_models.churn_model"
    )

    mlflow.log_param("model_type", "LogisticRegression")
```

ğŸ“Œ **Exam**

* Spark ML model â†’ `mlflow.spark.log_model`
* UC model name is **3-level namespace**

---

# 7ï¸âƒ£ Create Feature Store Table (Unity Catalog)

## Create features DataFrame

```python
features_df = df.select(
    "customer_id",
    "age",
    "income",
    "country"
)
```

---

## Register Feature Store table

```python
from databricks.feature_store import FeatureStoreClient

fs = FeatureStoreClient()

fs.create_table(
    name="main.ml_features.customer_features",
    primary_keys=["customer_id"],
    df=features_df,
    description="Customer features for churn prediction"
)
```

---

## Write / Update features

```python
fs.write_table(
    name="main.ml_features.customer_features",
    df=features_df,
    mode="merge"
)
```

ğŸ“Œ **Exam**

> Feature Store tables are **Delta tables + metadata**

---

# 8ï¸âƒ£ Train Using Feature Store (Reusable Training)

```python
from databricks.feature_store import FeatureLookup

lookups = [
    FeatureLookup(
        table_name="main.ml_features.customer_features",
        lookup_key="customer_id"
    )
]

training_set = fs.create_training_set(
    df=df.select("customer_id", label_col),
    feature_lookups=lookups,
    label=label_col
)

training_df = training_set.load_df()
```

Now reuse **same pipeline**:

```python
model = pipeline.fit(training_df)
```

ğŸ“Œ **Exam**

> Feature Store prevents training-serving skew

---

# 9ï¸âƒ£ Re-Training with Same Features (MLOps Pattern)

* Feature logic unchanged
* New data arrives
* Just rerun training pipeline
* Log new run â†’ register new version

âœ” **Model promotion, not code promotion**

---

# ğŸ”Ÿ Batch Inference Using Feature Store

```python
scoring_df = spark.read.table("main.raw.new_customers")

scoring_set = fs.create_training_set(
    df=scoring_df.select("customer_id"),
    feature_lookups=lookups
)

scoring_features = scoring_set.load_df()

predictions = model.transform(scoring_features)
```

---

# 1ï¸âƒ£1ï¸âƒ£ Load Registered Model for Inference

```python
import mlflow.spark

model_uri = "models:/main.ml_models.churn_model@Champion"
model = mlflow.spark.load_model(model_uri)

preds = model.transform(scoring_features)
```

ğŸ“Œ **Exam**

> Use aliases (`Champion`) instead of versions

---

# 1ï¸âƒ£2ï¸âƒ£ Promote Model Using Alias (Champion / Challenger)

```python
from mlflow.tracking import MlflowClient

client = MlflowClient()

client.set_registered_model_alias(
    name="main.ml_models.churn_model",
    alias="Champion",
    version="2"
)
```

---

# 1ï¸âƒ£3ï¸âƒ£ What You Have Achieved (EXAM SUMMARY)

âœ” Spark ML pipeline
âœ” Numeric + categorical processing
âœ” One-hot encoding (correct usage)
âœ” Feature Store in Unity Catalog
âœ” MLflow tracking
âœ” UC model registry
âœ” Reusable training
âœ” Reusable inference
âœ” Champion / challenger MLOps

---

# ğŸ§  FINAL MEMORY BLOCK (LOCK THIS IN)

```
Spark DF â†’ Pipeline
Pipeline â†’ MLflow
Features â†’ Feature Store (UC)
Model â†’ UC Registry
Reuse â†’ No skew
Alias â†’ Champion
```

---

## ğŸ¯ This single example covers ~40â€“50% of the exam concepts

If you want next, I can:

* Convert this into **MCQs**
* Show **AutoML â†’ Feature Store â†’ Serving**
* Do a **full mock exam**
* Or give you a **1-page final revision sheet**

Just tell me ğŸ‘
